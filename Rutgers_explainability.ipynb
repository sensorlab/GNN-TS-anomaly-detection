{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b41c4e7-b734-41c9-9976-dd9fb20da545",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "all of the packages needed in this notebook and their versions. Restart the kernel after runing this cell.\n",
    "\"\"\"\n",
    "\n",
    "!mamba install tensorflow-gpu==2.11.0 -y -q\n",
    "!mamba install pytorch-cuda=11.6 -c pytorch -c conda-forge -c nvidia -y -q\n",
    "!mamba install -c conda-forge pyts==0.12.0 -y -q\n",
    "!pip install torch==1.13 torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116 -q\n",
    "!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-1.13.1+cu116.html -q\n",
    "\n",
    "!pip install ts2vg==1.2.1 -q\n",
    "!pip install pytorch_lightning==1.9.1 -q\n",
    "!pip install torchsummary==1.5.1 -q\n",
    "!pip install dvclive==2.0.2 -q\n",
    "\n",
    "!pip install -e GraphXAI/ #==0.1\n",
    "!pip install pyedflib==0.1.33 -q\n",
    "!pip install mne==1.4.2 -q\n",
    "!pip install ipdb==0.13.13 -q\n",
    "!pip install pgmpy==0.1.23 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c6b761a-f8a7-41d5-a45b-03414fd9284f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "from pyts.image import MarkovTransitionField\n",
    "\n",
    "from ts2vg import NaturalVG\n",
    "from ts2vg import HorizontalVG\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "def get_Rutgers():\n",
    "    \"\"\"\n",
    "    Gets X, labels Y for graph clasification, and mask of Y for every sample for node classification, of a chosen Rutgers time series\n",
    "\n",
    "    Affected by:\n",
    "        Config[\"graph\"][\"len_type\"]\n",
    "        Config[\"graph\"][\"data_path\"]\n",
    "        Config[\"graph\"][\"folder_name\"]\n",
    "        Config[\"graph\"][\"properties_name\"]\n",
    "        Config[\"graph\"][\"mask_name\"]\n",
    "        \n",
    "    Returns:\n",
    "       _X: a 2D array containing the time steps of the rutgers dataset\n",
    "       mask: a 2D array containing the mask for node classification for the rutgers dataset\n",
    "       Y: a 1D array containing the mask for graph classification for the rutgers dataset\n",
    "    \"\"\"\n",
    "    # for the path containing time series, all with the same number of samples\n",
    "    if Config[\"graph\"][\"len_type\"] == \"un/cut\":\n",
    "        df = pd.read_csv(Config[\"graph\"][\"data_path\"] + Config[\"graph\"][\"folder_name\"])  \n",
    "        del df['Unnamed: 0']\n",
    "        df.index, df.columns = [range(df.index.size), range(df.columns.size)]\n",
    "        length_rss = int((df.columns.stop-2)/2)\n",
    "        \n",
    "        X = df.loc[:,df.columns[:length_rss]].to_numpy()\n",
    "        Y = df[length_rss+1].to_numpy(dtype=np.uint8)\n",
    "        mask = df.loc[:,df.columns[length_rss+2:]].to_numpy()\n",
    "        \n",
    "    # for the path containing time series, with diferent number of samples\n",
    "    elif Config[\"graph\"][\"len_type\"] == \"random\":\n",
    "        dataset_rss = np.load(Config[\"graph\"][\"data_path\"] + Config[\"graph\"][\"folder_name\"], allow_pickle=True)['arr_0']\n",
    "        dataset_properties = np.load(Config[\"graph\"][\"data_path\"] + Config[\"graph\"][\"properties_name\"], allow_pickle=True)['arr_0']\n",
    "        dataset_mask = np.load(Config[\"graph\"][\"data_path\"] + Config[\"graph\"][\"mask_name\"], allow_pickle=True)['arr_0']\n",
    "\n",
    "        for i in range(len(dataset_properties)):\n",
    "            dataset_properties[i,1] = int(dataset_properties[i,1])\n",
    "        \n",
    "        X = dataset_rss \n",
    "        mask = dataset_mask \n",
    "        Y = dataset_properties[:,2] \n",
    "        \n",
    "    return X, mask, Y\n",
    "\n",
    "def get_matrix(X_current):\n",
    "    \"\"\"\n",
    "    This function gets the adjacency matrix through either visibility, MTF, or dual VG graph\n",
    "    \n",
    "    Affected:\n",
    "        Config[\"graph\"][\"type\"]\n",
    "        Config[\"graph\"][\"VG\"][\"edge_type\"]\n",
    "        Config[\"graph\"][\"VG\"][\"edge_dir\"]\n",
    "        Config[\"graph\"][\"MTF\"][\"num_bins\"]\n",
    "        \n",
    "    Args:\n",
    "        X_current: a 1D array usually containing time series values\n",
    "    \n",
    "    Returns:\n",
    "        adj_mat: a list of adjacency matrices\n",
    "    \"\"\"\n",
    "    adj_mat = []\n",
    "    \n",
    "    if Config[\"graph\"][\"type\"] in (\"VG\", \"Dual_VG\"):\n",
    "        VGConfig = Config[\"graph\"][\"VG\"]\n",
    "        \n",
    "        if VGConfig[\"edge_type\"] == \"natural\":\n",
    "            g = NaturalVG(weighted=VGConfig[\"distance\"])\n",
    "        elif VGConfig[\"edge_type\"] == \"horizontal\":\n",
    "            g = HorizontalVG(weighted=VGConfig[\"distance\"])\n",
    "\n",
    "        g.build(X_current)\n",
    "\n",
    "        adj_mat_vis = np.zeros([len(X_current), len(X_current)], dtype='float')\n",
    "        for x, y, q in g.edges:\n",
    "            adj_mat_vis[x, y] = q\n",
    "            if VGConfig[\"edge_dir\"] == \"undirected\":\n",
    "                adj_mat_vis[y, x] = q\n",
    "        \n",
    "        adj_mat.append(adj_mat_vis)\n",
    "        \n",
    "    elif Config[\"graph\"][\"type\"] == \"MTF\":\n",
    "        n_bins = Config[\"graph\"][\"MTF\"][\"num_bins\"]\n",
    "        if n_bins == \"auto\":\n",
    "            n_bins = len(X_current)\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        MTF = MarkovTransitionField(n_bins=n_bins)\n",
    "        X_gaf_MTF_temp = MTF.fit_transform(X_current.reshape(1, -1))\n",
    "        adj_mat.append(X_gaf_MTF_temp[0])\n",
    "    \n",
    "    return adj_mat\n",
    "    \n",
    "def adjToEdgidx(adj_mat):\n",
    "    \"\"\"\n",
    "    This function creates edge indexes and weights for a given matrix\n",
    "    \n",
    "    Args:\n",
    "        adj_mat: a 2D array\n",
    "\n",
    "    Returns:\n",
    "        edge_index: a 2D torch array that indicates the connected values\n",
    "        edge_weight: a 1D array of weights that represent the absolute distance between connected nodes or values in the time series\n",
    "    \"\"\"\n",
    "    edge_index = torch.from_numpy(adj_mat[0]).nonzero().t().contiguous()\n",
    "    row, col = edge_index\n",
    "    edge_weight = adj_mat[0][row, col]\n",
    "    return edge_index, edge_weight\n",
    "\n",
    "def adjToEdgidx_Dual_VG(X_current):\n",
    "    \"\"\"\n",
    "    Creates a dual visibility graph by first creating a directed VG from one side and then flipping and running the get_matrix function again.\n",
    "    By doing this, we join these two graphs and obtain a dual VG.\n",
    "\n",
    "    Args:\n",
    "        X_current: 1D array usually containing time series values\n",
    "\n",
    "    Returns:\n",
    "        edge_index: 2D torch array that defines the connected values\n",
    "        edge_weight: 2D array of weights that represent the absolute distance between every node or value in the time series\n",
    "    \"\"\"\n",
    "    pos_adj_mat_vis = get_matrix(X_current)[0]\n",
    "    neg_adj_mat_vis = get_matrix(-X_current)[0]\n",
    "    edge_index = torch.from_numpy(pos_adj_mat_vis + neg_adj_mat_vis).nonzero().t().contiguous()\n",
    "\n",
    "    # Join two edge_weight arrays\n",
    "    row, col = edge_index\n",
    "    edge_weight = np.zeros([len(row), 2], dtype='float')\n",
    "    edge_weight[:, 0] = pos_adj_mat_vis[row, col]\n",
    "    edge_weight[:, 1] = neg_adj_mat_vis[row, col]\n",
    "\n",
    "    return edge_index, edge_weight\n",
    "    \n",
    "def create_graph(output, X, mask, Y):\n",
    "    \"\"\"\n",
    "    Creates a graph in the torch geometric Data format, containing the node values x, mask values for training, testing, and validation, edge indexes, and edge attributes.\n",
    "\n",
    "    Affected by:\n",
    "        Config[\"graph\"][\"type\"]\n",
    "        Config[\"graph\"][\"classif\"]\n",
    "    \n",
    "    Args:\n",
    "        output: Dataset of multiple graphs (optional). New graph will be appended to this dataset.\n",
    "        X: Node values (integer).\n",
    "        mask: 1D array representing the mask.\n",
    "\n",
    "    Returns:\n",
    "        output: Updated dataset of multiple or singular graph.\n",
    "    \"\"\"\n",
    "    if Config[\"graph\"][\"type\"] in (\"VG\", \"MTF\"):\n",
    "        edge_index, edge_weight = adjToEdgidx(get_matrix(X))\n",
    "    elif Config[\"graph\"][\"type\"] == \"Dual_VG\":\n",
    "        edge_index, edge_weight = adjToEdgidx_Dual_VG(X)\n",
    "    \n",
    "    \n",
    "    \n",
    "    x = torch.unsqueeze(torch.tensor(X, dtype=torch.double), 1).clone().detach()\n",
    "    edge_index = edge_index.clone().detach().to(torch.int64)\n",
    "    edge_attr = torch.unsqueeze(torch.tensor(edge_weight, dtype=torch.double), 1).clone().detach()\n",
    "    \n",
    "    if Config[\"graph\"][\"classif\"] == \"graph\": # for graph classification\n",
    "        y = torch.tensor(Y, dtype=torch.long)\n",
    "    elif Config[\"graph\"][\"classif\"] == \"node\":# for node classification \n",
    "        y = torch.unsqueeze(torch.tensor(mask, dtype=torch.double),1)\n",
    "\n",
    "\n",
    "    output.append(Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b41ce719-b5a7-4f92-b61d-92a96e410d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "def Rutgers_graph():\n",
    "    \"\"\"\n",
    "    Performs a pipeline of operations to process the Rutgers dataset for a given utility.\n",
    "\n",
    "    Affected by:\n",
    "        Config[\"graph\"]\n",
    "\n",
    "    Returns:\n",
    "        output: A list containing the processed graph data.\n",
    "        class_weights: Torch tensor containing the computed class weights.\n",
    "    \"\"\"\n",
    "    # creates X, mask of lables and Y as a graph lable\n",
    "    X, mask, Y = get_Rutgers()\n",
    "    \n",
    "    # here graphs are appended to the dataset\n",
    "    dataset = []\n",
    "    for i in range(len(X)):\n",
    "        dataset = create_graph(dataset, X[i], mask[i],Y[i])\n",
    "        \n",
    "    # join all lables into a 1D array\n",
    "    all_x = []\n",
    "    for obj in dataset:\n",
    "        all_x.append(obj.y.numpy())\n",
    "    all_x = np.array(all_x).reshape(-1)\n",
    "    \n",
    "    # all_x = np.reshape(np.concatenate([obj.y for obj in dataset]), (-1,))\n",
    "    \n",
    "    class_weights = torch.tensor(class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                                                    classes=np.unique(all_x),\n",
    "                                                                    y=all_x))\n",
    "    if Config[\"graph\"][\"classif\"] == \"node\":\n",
    "        class_weights =torch.tensor([class_weights[1]/class_weights[0]])\n",
    "\n",
    "    return dataset, class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b18e7af6-b716-4e04-bfda-3c6785ed5bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from dvclive.lightning import DVCLiveLogger\n",
    "\n",
    "from torch.nn import Sequential, BatchNorm1d, ReLU, Linear, CrossEntropyLoss, BCEWithLogitsLoss\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GINEConv, GATConv, global_max_pool\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\"\"\"\n",
    "This is the definition of a model arhitecture created for graph classification using GINEConv layers\n",
    "\"\"\" \n",
    "class GINE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        if Config[\"graph\"][\"type\"] in (\"MTF\", \"VG\"):\n",
    "            edge_dim = 1\n",
    "        elif Config[\"graph\"][\"type\"] in (\"dual_VG\"):\n",
    "            edge_dim = 2\n",
    "\n",
    "        dim_h = 32\n",
    "        # Define GINEConv and Linear layers\n",
    "        self.conv1 = GINEConv(\n",
    "            Sequential(Linear(dim_h, dim_h),\n",
    "                       BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()), edge_dim=edge_dim)\n",
    "\n",
    "        self.conv2 = GINEConv(\n",
    "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()), edge_dim=edge_dim)\n",
    "\n",
    "        self.conv3 = GINEConv(\n",
    "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()), edge_dim=edge_dim)\n",
    "\n",
    "        self.conv4 = GINEConv(\n",
    "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()), edge_dim=edge_dim)\n",
    "\n",
    "        self.conv5 = GINEConv(\n",
    "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "                       Linear(dim_h, dim_h), ReLU()), edge_dim=edge_dim)\n",
    "\n",
    "\n",
    "        self.lin1 = Linear(dim_h*5, dim_h*5)\n",
    "        self.lin2 = Linear(dim_h*5, 5)\n",
    "\n",
    "    def forward(self, x, edge_index, forward_kwargs):\n",
    "        edge_weight = forward_kwargs['edge_weight']#.to(device)\n",
    "        batch = forward_kwargs['batch']#.to(device)\n",
    "        x = x#.to(device)\n",
    "        edge_index = edge_index#.to(device)\n",
    "\n",
    "        # Node embeddings \n",
    "        h1 = self.conv1(x, edge_index, edge_attr=edge_weight)\n",
    "        h2 = self.conv2(h1, edge_index, edge_attr=edge_weight)\n",
    "        h3 = self.conv3(h2, edge_index, edge_attr=edge_weight)\n",
    "        h4 = self.conv4(h3, edge_index, edge_attr=edge_weight)\n",
    "        h5 = self.conv5(h4, edge_index, edge_attr=edge_weight)\n",
    "\n",
    "        # Graph-level readout\n",
    "        h1 = global_max_pool(h1, batch)\n",
    "        h2 = global_max_pool(h2, batch)\n",
    "        h3 = global_max_pool(h3, batch)\n",
    "        h4 = global_max_pool(h4, batch)\n",
    "        h5 = global_max_pool(h5, batch)\n",
    "\n",
    "        # Concatenate graph embeddings\n",
    "        h = torch.cat((h1, h2, h3, h4, h5), dim=1)\n",
    "\n",
    "        # Classifier\n",
    "        h = self.lin1(h)\n",
    "        h = h.relu()\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "        h = self.lin2(h)\n",
    "\n",
    "        return h\n",
    "\n",
    "\"\"\"\n",
    "This is the definition of a model arhitecture created for node classification using GATConv and Linear layers\n",
    "\"\"\" \n",
    "class GAT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Define GATConv and Linear layers\n",
    "        self.conv1 = GATConv(1, 32, heads=4)\n",
    "        self.lin1 = torch.nn.Linear(1, 4 * 32)\n",
    "        self.conv2 = GATConv(4 * 32, 32, heads=4)\n",
    "        self.lin2 = torch.nn.Linear(4 * 32, 4 * 32)\n",
    "        self.conv3 = GATConv(4 * 32, 1, heads=6,concat=False)\n",
    "        self.lin3 = torch.nn.Linear(4 * 32, 1)\n",
    "\n",
    "    def forward(self, x, edge_index, forward_kwargs):\n",
    "        edge_weight = forward_kwargs['edge_weight']#.to(device)\n",
    "        batch = forward_kwargs['batch']#.to(device)\n",
    "        x = x#.to(device)\n",
    "        edge_index = edge_index#.to(device)\n",
    "        # Process input data through convolutional and linear layers\n",
    "        x = F.elu(self.conv1(x, edge_index) + self.lin1(x))\n",
    "        x = F.elu(self.conv2(x, edge_index) + self.lin2(x))\n",
    "        x = self.conv3(x, edge_index) + self.lin3(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "class NGClassifier is a Pytorch Lightning class that is used to train, validate and test a model arhitecture for graph or node classificaiton\n",
    "\"\"\" \n",
    "class NGClassifier(pl.LightningModule):\n",
    "    def __init__(self, class_weights, model,Config):\n",
    "        super().__init__()\n",
    "        self.class_weights = class_weights\n",
    "        self.model = model\n",
    "        self.Config = Config\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        Configures the optimizer for training the model.\n",
    "        \n",
    "        Affected by:\n",
    "            Config[\"graph\"][\"learning_rate\"]\n",
    "        \n",
    "        Returns:\n",
    "            optimizer: The configured optimizer.\n",
    "        \"\"\"\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.Config[\"model\"][\"learning_rate\"], weight_decay=5e-4)\n",
    "        return optimizer\n",
    "    \n",
    "    def acc_pred(self, out, y):\n",
    "        \"\"\"\n",
    "        Calculates the accuracy and predicted labels that the trained model has generated.\n",
    "\n",
    "        Affected by:\n",
    "            Config[\"graph\"][\"classif\"]\n",
    "\n",
    "        Args:\n",
    "            out: Output values from a trained model.\n",
    "            y: True labels.\n",
    "\n",
    "        Returns:\n",
    "            accuracy: Accuracy of the model's predictions.\n",
    "            pred: Predicted labels.\n",
    "        \"\"\"\n",
    "        if self.Config[\"graph\"][\"classif\"] == \"graph\":\n",
    "            pred = out.argmax(-1)\n",
    "            accuracy = (pred == y).sum() / pred.shape[0]\n",
    "            \n",
    "        elif self.Config[\"graph\"][\"classif\"] == \"node\":\n",
    "            preds = []\n",
    "            preds.append((out > 0).float().cpu())     \n",
    "            pred = torch.cat(preds, dim=0)\n",
    "            accuracy = (pred == y.cpu()).sum() / pred.shape[0]\n",
    "        \n",
    "        return accuracy, pred\n",
    "    \n",
    "    def loss_function_selection(self):\n",
    "        \"\"\"\n",
    "        Choses a loss function depending on what type of classification is happening (graph or node classification)\n",
    "\n",
    "        Affected by:\n",
    "            Config[\"graph\"][\"classif\"]\n",
    "\n",
    "        Returns:\n",
    "            a loss funciton \n",
    "        \"\"\"\n",
    "        if self.Config[\"graph\"][\"classif\"] == \"graph\":\n",
    "            return CrossEntropyLoss(weight=self.class_weights).to(device)\n",
    "        elif self.Config[\"graph\"][\"classif\"] == \"node\":\n",
    "            return BCEWithLogitsLoss(weight=self.class_weights).to(device)\n",
    "            \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Performs a single training step on the given batch of train data.\n",
    "\n",
    "        Args:\n",
    "            train_batch: Input data for the training step.\n",
    "            batch_idx: Index of the current batch.\n",
    "\n",
    "        Returns:\n",
    "            train_loss: Loss value for the training step.\n",
    "        \"\"\"\n",
    "        forward_kwargs={\n",
    "            'edge_weight': train_batch.edge_attr,\n",
    "            'batch' : train_batch.batch\n",
    "        }       \n",
    "        \n",
    "        out = self.model(x=train_batch.x,edge_index=train_batch.edge_index,forward_kwargs=forward_kwargs)\n",
    "        loss_function = self.loss_function_selection()\n",
    "        train_loss = loss_function(out, train_batch.y)\n",
    "        \n",
    "        return train_loss\n",
    "    \n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Performs a single validation step on the given batch of validation data. It logs val_loss and accuracy for early stoping and saving the best trained models\n",
    "        \n",
    "        Args:\n",
    "            val_batch: Test data for the current batch.\n",
    "            batch_idx: Index of the current batch.\n",
    "        \"\"\"\n",
    "        \n",
    "        forward_kwargs={\n",
    "            'edge_weight': val_batch.edge_attr,\n",
    "            'batch' : val_batch.batch\n",
    "        }\n",
    "        out = self.model(x=val_batch.x,edge_index=val_batch.edge_index,forward_kwargs=forward_kwargs)\n",
    "        loss_function = self.loss_function_selection()\n",
    "        val_loss = loss_function(out, val_batch.y)\n",
    "        \n",
    "        accuracy, pred = self.acc_pred(out, val_batch.y)\n",
    "        \n",
    "        self.log(\"val_loss\", val_loss)\n",
    "        self.log(\"val_acc\", accuracy)        \n",
    "    \n",
    "    def test_step(self, test_batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Performs a single test step on the given batch of test data. It calculates and logs the test accuracy and returns the predicted labels and ground truth labels.\n",
    "\n",
    "        Args:\n",
    "            test_batch: Test data for the current batch.\n",
    "            batch_idx: Index of the current batch.\n",
    "            \n",
    "        Returns:\n",
    "            pred: labels predicted by the model \n",
    "            test_batch.y: labels that are true for the tested graph\n",
    "        \"\"\"\n",
    "        # Sort the importance mask values and indices\n",
    "        value, index = self.imp_mask[batch_idx].sort() #descending=True\n",
    "\n",
    "        # Clone the input features and initialize the test mask\n",
    "        x_masked = test_batch.x.clone()\n",
    "        test_mask = torch.ones(1, 300 - int(self.imp_mask_index))\n",
    "\n",
    "        edge_index = test_batch.edge_index\n",
    "        edge_attr = test_batch.edge_attr\n",
    "\n",
    "        x_masked1 = x_masked\n",
    "\n",
    "        if self.imp_mask_index != 0:\n",
    "            # Select the top k important nodes to mask\n",
    "            index = index[-self.imp_mask_index:]\n",
    "\n",
    "            # Adjust the indices of the remaining nodes\n",
    "            for i in range(len(index)-1):\n",
    "                for j in range(i+1, len(index)):\n",
    "                    if index[i] < index[j]:\n",
    "                        index[j] = index[j] - 1\n",
    "\n",
    "            # Mask the selected nodes in x_masked and adjust the edge indices and attributes\n",
    "            for element in index[-self.imp_mask_index:]:\n",
    "                x_masked = torch.cat((x_masked[:element], x_masked[element+1:]))\n",
    "\n",
    "                # Adjust the node indices in edge_index for the masked node\n",
    "                masked_node_index = element\n",
    "                num_nodes_deleted = self.imp_mask_index\n",
    "                node_mask = (edge_index[0] != masked_node_index) & (edge_index[1] != masked_node_index)\n",
    "                edge_index = edge_index[:, node_mask]\n",
    "                edge_attr = edge_attr[node_mask]\n",
    "\n",
    "                # Adjust the remaining node indices in edge_index after the deleted nodes\n",
    "                edge_index[0] = torch.where(edge_index[0] > masked_node_index, edge_index[0] - 1, edge_index[0])\n",
    "                edge_index[1] = torch.where(edge_index[1] > masked_node_index, edge_index[1] - 1, edge_index[1])\n",
    "\n",
    "            # Adjust the batch size and node index for the masked node\n",
    "            batch_size = test_batch.batch.max().item() + 1\n",
    "            num_nodes = test_batch.num_nodes - self.imp_mask_index\n",
    "            test_batch.batch = test_batch.batch[torch.cat((index[:-self.imp_mask_index], torch.tensor([0] * self.imp_mask_index)))]\n",
    "\n",
    "        # Forward pass with masked input features and adjusted graph\n",
    "        x_masked2 = x_masked\n",
    "        forward_kwargs = {\n",
    "            'edge_weight': edge_attr,\n",
    "            'batch': torch.zeros(300 - self.imp_mask_index).to(device).to(torch.int64)\n",
    "        }\n",
    "        out = self.model(x=x_masked, edge_index=edge_index, forward_kwargs=forward_kwargs)\n",
    "\n",
    "        loss_function = CrossEntropyLoss(weight=self.class_weights).to(device)\n",
    "        test_loss = loss_function(out, test_batch.y)\n",
    "\n",
    "        pred = out.argmax(-1)\n",
    "        test_label = test_batch.y\n",
    "        \n",
    "        accuracy = (pred == test_label).sum().item() / pred.shape[0]\n",
    "\n",
    "        # Log the test accuracy\n",
    "        self.log(\"test_acc\", accuracy)\n",
    "\n",
    "        # Return the predicted labels and ground truth labels\n",
    "        return pred, test_label\n",
    "    \n",
    "    def test_epoch_end(self, outputs):\n",
    "        \"\"\"\n",
    "        This function receives accumulated predicted and true labels from the test_step and uses them on the confusion matrix and classification report that are than printed.\n",
    "        \n",
    "        Args:\n",
    "            outputs: Contains an array of predicted and an array of true lables\n",
    "\n",
    "        \"\"\"\n",
    "        global pred_array, true_array\n",
    "        true_array=[outputs[i][1].cpu().numpy() for i in range(len(outputs))]\n",
    "        pred_array = [outputs[i][0].cpu().numpy() for i in range(len(outputs))]  \n",
    "        print(confusion_matrix(true_array, pred_array))\n",
    "        print(classification_report(true_array, pred_array))\n",
    "        \n",
    "        \n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function for training.\n",
    "\n",
    "    Affected by:\n",
    "        Config[\"graph\"][\"classif\"]\n",
    "        Config[\"model\"]\n",
    "    \"\"\"\n",
    "\n",
    "    Config[\"graph\"][\"classif\"] = \"graph\"\n",
    "    Config[\"graph\"][\"type\"] = \"VG\"\n",
    "    Config[\"model\"][\"SEED\"] = 280\n",
    "\n",
    "    global device\n",
    "    # initiate callback functions, DVC, Seed and device\n",
    "    early_stop = EarlyStopping(monitor='val_acc',patience=Config[\"model\"][\"patience\"], strict=False,verbose=False, mode='max')\n",
    "    val_checkpoint_best_loss = ModelCheckpoint(filename=\"best_loss\", monitor = \"val_loss\", mode=\"min\")\n",
    "    val_checkpoint_best_acc = ModelCheckpoint(filename=\"best_acc\", monitor = \"val_acc\", mode=\"max\")\n",
    "    logger = DVCLiveLogger() # the bonus of using DVCLiveLogger() is that you can visualise the validation accuracy live in dvclive/report.html\n",
    "    \n",
    "    torch.manual_seed(Config[\"model\"][\"SEED\"])\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # creates dataset containing graphs and the overall class_weights\n",
    "    output, class_weights = Rutgers_graph() \n",
    "\n",
    "    # sets the train, validation and test sizes and atributes a number of time series coresponding to those sizes to every DataLoader\n",
    "    train_size = int(Config[\"model\"][\"train/val/test\"][\"train\"] * len(output))\n",
    "    val_size = int(Config[\"model\"][\"train/val/test\"][\"val\"]*len(output))\n",
    "    test_size = len(output) - (val_size + train_size) \n",
    "    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(output, [train_size, val_size, test_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=Config[\"model\"][\"batch_size\"], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=int(Config[\"model\"][\"batch_size\"]/2), shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "    if Config[\"model\"][\"status\"] == \"train\":\n",
    "        model = NGClassifier(class_weights=class_weights,model=GINE(),Config=Config).double()\n",
    "\n",
    "        trainer = pl.Trainer(logger=logger,accelerator='gpu',devices=1)\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    elif Config[\"model\"][\"status\"] == \"test\": #gets the accuracy for anomaly type selected in Config[\"importance\"][\"anomaly_type\"] and number of nodes deleted from graph in Config[\"importance\"][\"take_out\"]\n",
    "\n",
    "        model = NGClassifier.load_from_checkpoint(Config[\"importance\"][\"ckpt_path\"],class_weights=class_weights,model=GINE(),Config=Config).double() #GINE is for graph classification\n",
    "        importance_scores=create_node_importance_array(test_loader,model.model) # Importance scores for every graph in the testing loader\n",
    "\n",
    "        test_loader.dataset.indices = importance_scores[Config[\"importance\"][\"anomaly_type\"]][2]\n",
    "\n",
    "        model.imp_mask_index = Config[\"importance\"][\"take_out\"]\n",
    "        model.imp_mask = importance_scores[Config[\"importance\"][\"anomaly_type\"]][Config[\"importance\"][\"importance\"]] # importance_scores[1][0] is for my importance matrix, while importance_scores[1][1] is the exp importance matrix from graphxai\n",
    "\n",
    "        trainer = pl.Trainer(logger=False,accelerator='gpu',devices=1)\n",
    "\n",
    "        trainer.test(model, test_loader)\n",
    "    elif Config[\"model\"][\"status\"] == \"test_auto\": #gets the array where num_of_anom are anomaly types and num_of_imp is the number of steps it takes where it deletes the next 30 most important nodes  \n",
    "\n",
    "        model = NGClassifier.load_from_checkpoint(Config[\"importance\"][\"ckpt_path\"],class_weights=class_weights,model=GINE(),Config=Config).double() #GINE is for graph classification\n",
    "        importance_scores=create_node_importance_array(test_loader,model.model)\n",
    "        \n",
    "        global imp_results, result_array\n",
    "        \n",
    "        num_of_anom = 5 # number of anomalies\n",
    "        num_of_imp = 81 # number of times nodes will be taken out - coresponding to \"num_of_nodes\". So max nodes that will be taken out is (\"num_of_imp\" -1) * \"num_of_nodes\"\n",
    "        num_of_nodes = 3 # number of nodes taken per step\n",
    "        result_array = [] # will become an array that tells how many nodes were taken in each step\n",
    "        \n",
    "        for i in range(num_of_imp):\n",
    "            result_array.append(num_of_nodes * i)\n",
    "\n",
    "        imp_results = np.zeros((num_of_anom, num_of_imp))\n",
    "        for i in range(num_of_anom):\n",
    "            test_loader.dataset.indices = importance_scores[i][2]\n",
    "            for j in range(num_of_imp):\n",
    "                model.imp_mask_index = j*num_of_nodes\n",
    "                model.imp_mask = importance_scores[i][Config[\"importance\"][\"importance\"]] \n",
    "\n",
    "                trainer = pl.Trainer(logger=False,accelerator='gpu',devices=1)\n",
    "                trainer.test(model, test_loader)\n",
    "                imp_results[i][j] = accuracy_score(true_array, pred_array)\n",
    "                print(accuracy_score(true_array, pred_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b608e44-5e95-4110-909f-d10268096ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(csv_name,true_array, pred_array):\n",
    "    \"\"\"\n",
    "    Generates a classification report based on the true and predicted arrays for a given utility.\n",
    "\n",
    "    Args:\n",
    "        csv_name: Name of the .csv file.\n",
    "        true_array: Array of true labels.\n",
    "        pred_array: Array of predicted labels.\n",
    "\n",
    "    Returns:\n",
    "        Saves the report as a CSV file.\n",
    "    \"\"\"\n",
    "    print(csv_name)\n",
    "    report = classification_report(true_array, pred_array, output_dict=True)\n",
    "    df = pd.DataFrame(report).transpose()\n",
    "    file_name = 'results_rutgers'+str(Config[\"model\"][\"SEED\"])+'/'+csv_name\n",
    "    df.to_csv(file_name +\".csv\")\n",
    "    shutil.make_archive(file_name, 'zip', file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728770de-936d-49d7-9284-99019b90a9bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98f57df9-5347-4ccd-967a-d2106af379ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from typing import Optional, Callable\n",
    "from torch_geometric.utils import k_hop_subgraph\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "from graphxai.explainers._base import _BaseExplainer\n",
    "from graphxai.utils import Explanation\n",
    "\n",
    "\n",
    "class GradExplainer(_BaseExplainer):\n",
    "    \"\"\"\n",
    "    Vanilla Gradient Explanation for GNNs\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): model on which to make predictions\n",
    "            The output of the model should be unnormalized class score.\n",
    "            For example, last layer = CNConv or Linear.\n",
    "        criterion (torch.nn.Module): loss function\n",
    "    \"\"\"\n",
    "    def __init__(self, model: torch.nn.Module, \n",
    "            criterion: Callable[[torch.Tensor, torch.Tensor], torch.Tensor]):\n",
    "        super().__init__(model)\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def get_explanation_node(self, node_idx: int, x: torch.Tensor,\n",
    "                             edge_index: torch.Tensor,\n",
    "                             label: Optional[torch.Tensor] = None,\n",
    "                             num_hops: Optional[int] = None,\n",
    "                             aggregate_node_imp = torch.sum,\n",
    "                             y = None,\n",
    "                             forward_kwargs: dict = {}, **_) -> Explanation:\n",
    "        \"\"\"\n",
    "        Explain a node prediction.\n",
    "\n",
    "        Args:\n",
    "            node_idx (int): index of the node to be explained\n",
    "            x (torch.Tensor, [n x d]): node features\n",
    "            edge_index (torch.Tensor, [2 x m]): edge index of the graph\n",
    "            label (torch.Tensor, optional, [n x ...]): labels to explain\n",
    "                If not provided, we use the output of the model.\n",
    "                (:default: :obj:`None`)\n",
    "            num_hops (int, optional): number of hops to consider\n",
    "                If not provided, we use the number of graph layers of the GNN.\n",
    "                (:default: :obj:`None`)\n",
    "            aggregate_node_imp (function, optional): torch function that aggregates\n",
    "                all node importance feature-wise scores across the enclosing \n",
    "                subgraph. Must support `dim` argument. \n",
    "                (:default: :obj:`torch.sum`)\n",
    "            forward_kwargs (dict, optional): Additional arguments to model.forward \n",
    "                beyond x and edge_index. Must be keyed on argument name. \n",
    "                (default: :obj:`{}`)\n",
    "\n",
    "        :rtype: :class:`graphxai.Explanation`\n",
    "\n",
    "        Returns:\n",
    "            exp (:class:`Explanation`): Explanation output from the method.\n",
    "                Fields are:\n",
    "                `feature_imp`: :obj:`torch.Tensor, [features,]`\n",
    "                `node_imp`: :obj:`torch.Tensor, [nodes_in_khop, features]`\n",
    "                `edge_imp`: :obj:`None`\n",
    "                `enc_subgraph`: :obj:`graphxai.utils.EnclosingSubgraph`\n",
    "        \"\"\"\n",
    "        label = self._predict(x, edge_index,\n",
    "                              forward_kwargs=forward_kwargs) if label is None else label\n",
    "        num_hops = self.L if num_hops is None else num_hops\n",
    "\n",
    "        khop_info = subset, sub_edge_index, mapping, _ = \\\n",
    "            k_hop_subgraph(node_idx, num_hops, edge_index,\n",
    "                           relabel_nodes=True, num_nodes=x.shape[0])\n",
    "        sub_x = x[subset]\n",
    "\n",
    "        self.model.eval()\n",
    "        sub_x.requires_grad = True\n",
    "        output = self.model(sub_x, sub_edge_index)\n",
    "        loss = self.criterion(output[mapping], label[mapping])\n",
    "        loss.backward()\n",
    "\n",
    "        feature_imp = sub_x.grad[torch.where(subset == node_idx)].squeeze(0)\n",
    "        node_imp = aggregate_node_imp(sub_x.grad, dim = 1)\n",
    "\n",
    "        exp = Explanation(\n",
    "            feature_imp = feature_imp, #[score_1, ]\n",
    "            node_imp = node_imp, #[score_1, score_2, ...] [[], []] NxF\n",
    "            node_idx = node_idx\n",
    "        )\n",
    "\n",
    "        exp.set_enclosing_subgraph(khop_info)\n",
    "\n",
    "        return exp\n",
    "\n",
    "    def get_explanation_graph(self, \n",
    "                                x: torch.Tensor, \n",
    "                                edge_index: torch.Tensor,\n",
    "                                label: torch.Tensor, \n",
    "                                aggregate_node_imp = torch.sum,\n",
    "                                forward_kwargs: dict = {}) -> Explanation:\n",
    "        \"\"\"\n",
    "        Explain a whole-graph prediction.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor, [n x d]): node features\n",
    "            edge_index (torch.Tensor, [2 x m]): edge index of the graph\n",
    "            label (torch.Tensor, [n x ...]): labels to explain\n",
    "            aggregate_node_imp (function, optional): torch function that aggregates\n",
    "                all node importance feature-wise scores across the graph. \n",
    "                Must support `dim` argument. (:default: :obj:`torch.sum`)\n",
    "            forward_kwargs (dict, optional): additional arguments to model.forward\n",
    "                beyond x and edge_index\n",
    "\n",
    "        :rtype: :class:`graphxai.Explanation`\n",
    "\n",
    "        Returns:\n",
    "            exp (:class:`Explanation`): Explanation output from the method. \n",
    "                Fields are:\n",
    "                `feature_imp`: :obj:`None`\n",
    "                `node_imp`: :obj:`torch.Tensor, [num_nodes, features]`\n",
    "                `edge_imp`: :obj:`None`\n",
    "                `graph`: :obj:`torch_geometric.data.Data`\n",
    "        \"\"\"\n",
    "\n",
    "        self.model.eval()\n",
    "        x.requires_grad = True\n",
    "        output = self.model(x, edge_index, **forward_kwargs)[0]\n",
    "        loss = self.criterion(output, label)\n",
    "        loss.backward()\n",
    "\n",
    "        node_imp = aggregate_node_imp(x.grad, dim = 1)\n",
    "\n",
    "        exp = Explanation(\n",
    "            node_imp = node_imp\n",
    "        )\n",
    "\n",
    "        exp.set_whole_graph(Data(x=x, edge_index=edge_index))\n",
    "\n",
    "        return exp\n",
    "\n",
    "    def get_explanation_link(self):\n",
    "        \"\"\"\n",
    "        Explain a link prediction.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ecfac0b6-3c47-4cbe-a465-255cc692851a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_important_nodes(model, data):\n",
    "    \"\"\"\n",
    "    calculates important nodes through gradient and gets the explanation from the GNNExplainer.\n",
    "    \n",
    "    Args:\n",
    "        model: trained model\n",
    "        data: test data that will be run on the model\n",
    "        \n",
    "    returns:\n",
    "        node_importance: node importance scores calculated through gradient \n",
    "        exp: a class created by GraphXAI that contains important node scores that also works around gradient\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Enable gradient calculation\n",
    "    data.x.requires_grad = True\n",
    "    \n",
    "    forward_kwargs={\n",
    "        'edge_weight': data.edge_attr,#.to(device),\n",
    "        'batch' : torch.tensor([0])#.to(device)\n",
    "    }\n",
    "    # Forward pass\n",
    "    # print(data.x.device,data.edge_index.device,forward_kwargs['edge_weight'].device,forward_kwargs['batch'].device,model.device)\n",
    "    out = model(x=data.x,edge_index=data.edge_index,forward_kwargs=forward_kwargs)\n",
    "    # Calculate the loss (e.g., cross-entropy) based on the output and ground truth\n",
    "    loss = F.cross_entropy(out[0], torch.tensor(data.y))\n",
    "\n",
    "    # Compute gradients of the loss with respect to node embeddings\n",
    "    loss.backward()\n",
    "\n",
    "    # Calculate the node importance scores based on the gradients\n",
    "    node_importance = torch.abs(data.x.grad).sum(dim=1)\n",
    "    # exp=1\n",
    "    data.x.requires_grad = False\n",
    "    \n",
    "    fwargs={\n",
    "        'edge_weight': data.edge_attr,#.to(device),\n",
    "        'batch' : torch.tensor([0])#.to(device)\n",
    "    }\n",
    "\n",
    "    EXP = GradExplainer(model,torch.nn.CrossEntropyLoss())\n",
    "    exp = EXP.get_explanation_graph(\n",
    "        x=data.x,#.to(device)\n",
    "        edge_index=data.edge_index,#.to(device)\n",
    "        label = data.y,#.to(device)\n",
    "        forward_kwargs={'forward_kwargs':fwargs}\n",
    "    )\n",
    "\n",
    "    \n",
    "    return node_importance, exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e490b95-80b2-4288-9fdc-2e778457a492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_node_importance_array(test_loader,model):\n",
    "    \"\"\"\n",
    "    Creates an array that contains 7 parameters:\n",
    "        - node importance calculated by Gradient tehnique in function \"get_important_nodes\" \n",
    "        - node importance calculated by GraphXAI Gradient tehnique in function \"get_important_nodes\" \n",
    "        - indece of the graph whoes node importance was calculated \n",
    "        - node importance std\n",
    "        - node importance med\n",
    "        - node importance max\n",
    "        - node importance min\n",
    "    returns:\n",
    "        importance_score: array containing all 7 parameters for all graphs inside the test loader\n",
    "        \n",
    "    \"\"\"\n",
    "    import pickle\n",
    "    importance_score = [[[],[],[],[],[],[],[]],\n",
    "            [[],[],[],[],[],[],[]],\n",
    "            [[],[],[],[],[],[],[]],\n",
    "            [[],[],[],[],[],[],[]],\n",
    "            [[],[],[],[],[],[],[]]]\n",
    "    for i, data in enumerate(test_loader.dataset):\n",
    "        node_importance, exp = get_important_nodes(model, data)\n",
    "        std_data = np.std(np.array(node_importance.cpu()))\n",
    "        med_data = np.median(np.array(node_importance.cpu()))\n",
    "        max_data = np.max(np.array(node_importance.cpu()))\n",
    "        min_data = np.min(np.array(node_importance.cpu()))\n",
    "\n",
    "        importance_score[data.y][0].append(node_importance)\n",
    "        importance_score[data.y][1].append(exp.node_imp)\n",
    "        importance_score[data.y][2].append(test_loader.dataset.indices[i])\n",
    "        importance_score[data.y][3].append(std_data)\n",
    "        importance_score[data.y][4].append(med_data)\n",
    "        importance_score[data.y][5].append(max_data)\n",
    "        importance_score[data.y][6].append(min_data)\n",
    "        \n",
    "        \n",
    "        \n",
    "    return importance_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "317bfed5-7e7d-48aa-b633-5c4ef256ec86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_importance_impact():\n",
    "    \"\"\"\n",
    "    plots how the accuracy changes if we mask the important nodes by step of 30 nodes in the graph.\n",
    "    Can be run if we insert Config[\"model\"][\"status\"] = \"test_auto\" where the array of importance impact is given at the end\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    legend_labels = [\"no_anom\", \"non_recovery\", \"recovery\", \"instant\", \"slow\"]\n",
    "    colors =[\"red\",\"blue\",\"yellow\",\"green\", \"gray\"]\n",
    "\n",
    "    for i in range(5):\n",
    "        plt.plot(imp_results[i], label=legend_labels[i], color = colors[i])\n",
    "\n",
    "    plt.legend(labels=legend_labels)\n",
    "    plt.xlabel(\"Percentage of deleted nodes\")\n",
    "    plt.ylabel(\"F1 score\")\n",
    "    plt.savefig(name_of_folder + \".png\", format=\"png\")\n",
    "    plt.savefig(name_of_folder + \".svg\", format=\"svg\")\n",
    "    plt.show()\n",
    "    \n",
    "    df = pd.DataFrame(imp_results)\n",
    "    df.to_csv(name_of_folder+'.csv', index=False, decimal=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d855d2f0-18d3-4e4e-bddf-93aa05206d63",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58197309-086e-49c6-88bc-10826524b0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This list contains configurations for generating and training a graph from a Rutgers time series\n",
    "\"\"\"\n",
    "Config = {\n",
    "    \"graph\": {\n",
    "        \"data_path\" : \"datasets/Rutgers/\", # path to datasets\n",
    "        \"folder_name\" : \"dataset_uncut.csv\", #\"dataset_uncut.csv\", \"dataset_cut.csv\", \"dataset_rss.npz\", paths used for cut/uncut/random dataset\n",
    "        \"properties_name\" : \"dataset_properties.npz\",  # path to properties used for random \n",
    "        \"mask_name\" : \"dataset_mask.npz\", # path to mask dataset used for random\n",
    "        \"classif\": \"node\",  # Type of classification (node or graph)\n",
    "        \"len_type\" : \"un/cut\", #\"un/cut\", \"random\", the shape of data used in later paths \n",
    "        \"type\": \"VG\",  # Type of graph (MTF, VG, Dual_VG)\n",
    "        \"MTF\": {\n",
    "            \"num_bins\": \"auto\"  # Number of bins for MTF graph (integer or \"auto\")\n",
    "        },\n",
    "        \"VG\": {\n",
    "            \"edge_type\": \"natural\",  # Type of edge calculation for VG graph (natural or horizontal)\n",
    "            \"distance\": 'distance',  # Type of distance metric for VG graph\n",
    "                                    # (slope, abs_slope, distance, h_distance, v_distance, abs_v_distance)\n",
    "            \"edge_dir\": \"directed\"  # Directionality of edges in VG graph (undirected or directed)\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"model\": {\n",
    "        \"SEED\": 280,  # Random seed for reproducibility\n",
    "        \"learning_rate\": 0.005,  # Learning rate for training\n",
    "        \"batch_size\": 64,  # Batch size for training\n",
    "        \"range_epoch\": 200,  # Number of training epochs\n",
    "        \"save_file\": \"test_test\",  # File name for saving trained model\n",
    "        \"name_of_save\": \"test_u-time\",  # Name of the save (e.g., checkpoint name)\n",
    "        \"patience\": 500,  # Patience for early stopping\n",
    "        \"train/val/test\": {\n",
    "            \"train\": 0.8,  # Percentage of data used for training\n",
    "            \"val\": 0.04,  # Percentage of data used for validation\n",
    "            \"test\": 0.16  # Percentage of data used for testing\n",
    "        },\n",
    "        \"status\": \"test\" #, \"test\", \"full\",\"test_auto\" specifies what to do with the model\n",
    "    },\n",
    "    \"importance\":{\n",
    "        \"take_out\": 0, # number of nodes taken from the graph, which is then sent through the model to make a prediction\n",
    "        \"ckpt_path\": \"Trained_models_Rutgers/VG_dir_seed=6000/version_3.ckpt\", # path to the saved models parameters\n",
    "        \"anomaly_type\": 1,\n",
    "        \"importance\": 0 #0:refers to my importance matrix, while 1 refers to GraphXAI importance matrix\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f590a777-6ede-4304-ba89-424a44802c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This for loop runs the main function for graph generation, training and saving the testing results into a csv file.\n",
    "The trained model can be found in DvcLiveLogger/dvclive_run/checkpoints.\n",
    "\"\"\"\n",
    "#Most common Config parameters\n",
    "Config[\"graph\"][\"classif\"] = \"graph\"\n",
    "Config[\"graph\"][\"type\"] = \"VG\"\n",
    "Config[\"model\"][\"SEED\"] = 280\n",
    "Config[\"importance\"][\"anomaly_type\"] = 2\n",
    "Config[\"importance\"][\"importance\"] = 0\n",
    "Config[\"importance\"][\"take_out\"] = 3\n",
    "Config[\"importance\"][\"ckpt_path\"] = \"Trained_models_Rutgers/VG_seed=280/version_0.ckpt\"\n",
    "\n",
    "#For generating, training and testing the graph\n",
    "main()\n",
    "\n",
    "# Generate a classification report for the utility\n",
    "report(get_versions_TSSB()[utility][:-4],true_array, pred_array,2)\n",
    "\n",
    "plot_importance_impact()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d66ee82-fe6f-4583-bf13-5df3e9b00cec",
   "metadata": {},
   "source": [
    "# Importance score calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff2228b-dc7b-436e-9c73-f9f64574c230",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This for loop runs the main function for graph generation, training and saving the testing results into a csv file.\n",
    "The trained model can be found in DvcLiveLogger/dvclive_run/checkpoints.\n",
    "\"\"\"\n",
    "#Most common Config parameters\n",
    "Config[\"graph\"][\"classif\"] = \"graph\"\n",
    "Config[\"graph\"][\"type\"] = \"VG\"\n",
    "Config[\"model\"][\"SEED\"] = 280\n",
    "Config[\"importance\"][\"importance\"] = 0\n",
    "Config[\"importance\"][\"ckpt_path\"] = \"Trained_models_Rutgers/VG_seed=280/version_0.ckpt\" # path to the saved models parameters\n",
    "Config[\"model\"][\"status\"] = \"test_auto\"\n",
    "\n",
    "\n",
    "#For generating, training and testing the graph\n",
    "main()\n",
    "\n",
    "# Generate a classification report for the utility\n",
    "# report(get_versions_TSSB()[utility][:-4],true_array, pred_array,2)\n",
    "\n",
    "plot_importance_impact('VG_simple')\n",
    "\n",
    "df = pd.DataFrame({'Num_of_taken_nodes':np.array(result_array).reshape(81),'No_anomaly': imp_results[0], 'No_recovery':imp_results[1], 'Recovery':imp_results[2], 'Instant':imp_results[3], 'Slow':imp_results[4]})\n",
    "df.to_csv('VG_simple.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfcb811-06d6-451c-b8c6-07091484a152",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This for loop runs the main function for graph generation, training and saving the testing results into a csv file.\n",
    "The trained model can be found in DvcLiveLogger/dvclive_run/checkpoints.\n",
    "\"\"\"\n",
    "#Most common Config parameters\n",
    "Config[\"graph\"][\"classif\"] = \"graph\"\n",
    "Config[\"graph\"][\"type\"] = \"VG\"\n",
    "Config[\"model\"][\"SEED\"] = 280\n",
    "Config[\"importance\"][\"importance\"] = 1\n",
    "Config[\"importance\"][\"ckpt_path\"] = \"Trained_models_Rutgers/VG_seed=280/version_0.ckpt\" # path to the saved models parameters\n",
    "Config[\"model\"][\"status\"] = \"test_auto\"\n",
    "\n",
    "\n",
    "#For generating, training and testing the graph\n",
    "main()\n",
    "\n",
    "# Generate a classification report for the utility\n",
    "# report(get_versions_TSSB()[utility][:-4],true_array, pred_array,2)\n",
    "\n",
    "plot_importance_impact('VG_GraphXAI')\n",
    "\n",
    "df = pd.DataFrame({'Num_of_taken_nodes':np.array(result_array).reshape(81),'No_anomaly': imp_results[0], 'No_recovery':imp_results[1], 'Recovery':imp_results[2], 'Instant':imp_results[3], 'Slow':imp_results[4]})\n",
    "df.to_csv('VG_GraphXAI.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b1eb7a-d058-45bf-be42-cca5004b2db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This for loop runs the main function for graph generation, training and saving the testing results into a csv file.\n",
    "The trained model can be found in DvcLiveLogger/dvclive_run/checkpoints.\n",
    "\"\"\"\n",
    "#Most common Config parameters\n",
    "Config[\"graph\"][\"classif\"] = \"graph\"\n",
    "Config[\"graph\"][\"type\"] = \"MTF\"\n",
    "Config[\"model\"][\"SEED\"] = 280\n",
    "Config[\"importance\"][\"importance\"] = 0\n",
    "Config[\"importance\"][\"ckpt_path\"] = \"Trained_models_Rutgers/MTF_seed=280/version_0.ckpt\" # path to the saved models parameters\n",
    "Config[\"model\"][\"status\"] = \"test_auto\"\n",
    "\n",
    "\n",
    "#For generating, training and testing the graph\n",
    "main()\n",
    "\n",
    "# Generate a classification report for the utility\n",
    "# report(get_versions_TSSB()[utility][:-4],true_array, pred_array,2)\n",
    "\n",
    "plot_importance_impact('MTF_simple')\n",
    "\n",
    "df = pd.DataFrame({'Num_of_taken_nodes':np.array(result_array).reshape(81),'No_anomaly': imp_results[0], 'No_recovery':imp_results[1], 'Recovery':imp_results[2], 'Instant':imp_results[3], 'Slow':imp_results[4]})\n",
    "df.to_csv('MTF_simple.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed8af6a-42c3-41fa-95d8-edb3c6b4e25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This for loop runs the main function for graph generation, training and saving the testing results into a csv file.\n",
    "The trained model can be found in DvcLiveLogger/dvclive_run/checkpoints.\n",
    "\"\"\"\n",
    "#Most common Config parameters\n",
    "Config[\"graph\"][\"classif\"] = \"graph\"\n",
    "Config[\"graph\"][\"type\"] = \"MTF\"\n",
    "Config[\"model\"][\"SEED\"] = 280\n",
    "Config[\"importance\"][\"importance\"] = 1\n",
    "Config[\"importance\"][\"ckpt_path\"] = \"Trained_models_Rutgers/MTF_seed=280/version_0.ckpt\" # path to the saved models parameters\n",
    "Config[\"model\"][\"status\"] = \"test_auto\"\n",
    "\n",
    "\n",
    "#For generating, training and testing the graph\n",
    "main()\n",
    "\n",
    "# Generate a classification report for the utility\n",
    "# report(get_versions_TSSB()[utility][:-4],true_array, pred_array,2)\n",
    "\n",
    "plot_importance_impact('MTF_GraphXAI')\n",
    "\n",
    "df = pd.DataFrame({'Num_of_taken_nodes':np.array(result_array).reshape(81),'No_anomaly': imp_results[0], 'No_recovery':imp_results[1], 'Recovery':imp_results[2], 'Instant':imp_results[3], 'Slow':imp_results[4]})\n",
    "df.to_csv('MTF_GraphXAI.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b02edf-d820-4aa9-846a-77821a628c21",
   "metadata": {
    "tags": []
   },
   "source": [
    "# reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b544720a-93d6-4d4e-a24c-204d363f88dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This for loop runs the main function for graph generation, training and saving the testing results into a csv file.\n",
    "The trained model can be found in DvcLiveLogger/dvclive_run/checkpoints.\n",
    "\"\"\"\n",
    "#Most common Config parameters\n",
    "Config[\"graph\"][\"classif\"] = \"graph\"\n",
    "Config[\"graph\"][\"type\"] = \"VG\"\n",
    "Config[\"model\"][\"SEED\"] = 280\n",
    "Config[\"importance\"][\"importance\"] = 0\n",
    "Config[\"importance\"][\"ckpt_path\"] = \"Trained_models_Rutgers/VG_seed=280/version_0.ckpt\" # path to the saved models parameters\n",
    "Config[\"model\"][\"status\"] = \"test_auto\"\n",
    "\n",
    "\n",
    "#For generating, training and testing the graph\n",
    "main()\n",
    "\n",
    "# Generate a classification report for the utility\n",
    "# report(get_versions_TSSB()[utility][:-4],true_array, pred_array,2)\n",
    "\n",
    "plot_importance_impact('VG_simple_reverse')\n",
    "\n",
    "df = pd.DataFrame({'Num_of_taken_nodes':np.array(result_array).reshape(81),'No_anomaly': imp_results[0], 'No_recovery':imp_results[1], 'Recovery':imp_results[2], 'Instant':imp_results[3], 'Slow':imp_results[4]})\n",
    "df.to_csv('VG_simple_reverse.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b6c015-23be-4e2b-9ea8-5721c35dec65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This for loop runs the main function for graph generation, training and saving the testing results into a csv file.\n",
    "The trained model can be found in DvcLiveLogger/dvclive_run/checkpoints.\n",
    "\"\"\"\n",
    "#Most common Config parameters\n",
    "Config[\"graph\"][\"classif\"] = \"graph\"\n",
    "Config[\"graph\"][\"type\"] = \"VG\"\n",
    "Config[\"model\"][\"SEED\"] = 280\n",
    "Config[\"importance\"][\"importance\"] = 1\n",
    "Config[\"importance\"][\"ckpt_path\"] = \"Trained_models_Rutgers/VG_seed=280/version_0.ckpt\" # path to the saved models parameters\n",
    "Config[\"model\"][\"status\"] = \"test_auto\"\n",
    "\n",
    "\n",
    "#For generating, training and testing the graph\n",
    "main()\n",
    "\n",
    "# Generate a classification report for the utility\n",
    "# report(get_versions_TSSB()[utility][:-4],true_array, pred_array,2)\n",
    "\n",
    "plot_importance_impact('VG_GraphXAI_reverse')\n",
    "\n",
    "df = pd.DataFrame({'Num_of_taken_nodes':np.array(result_array).reshape(81),'No_anomaly': imp_results[0], 'No_recovery':imp_results[1], 'Recovery':imp_results[2], 'Instant':imp_results[3], 'Slow':imp_results[4]})\n",
    "df.to_csv('VG_GraphXAI_reverse.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142bb5ce-9bcf-4fb9-b50e-f0e09e5a0641",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This for loop runs the main function for graph generation, training and saving the testing results into a csv file.\n",
    "The trained model can be found in DvcLiveLogger/dvclive_run/checkpoints.\n",
    "\"\"\"\n",
    "#Most common Config parameters\n",
    "Config[\"graph\"][\"classif\"] = \"graph\"\n",
    "Config[\"graph\"][\"type\"] = \"MTF\"\n",
    "Config[\"model\"][\"SEED\"] = 280\n",
    "Config[\"importance\"][\"importance\"] = 0\n",
    "Config[\"importance\"][\"ckpt_path\"] = \"Trained_models_Rutgers/MTF_seed=280/version_0.ckpt\" # path to the saved models parameters\n",
    "Config[\"model\"][\"status\"] = \"test_auto\"\n",
    "\n",
    "\n",
    "#For generating, training and testing the graph\n",
    "main()\n",
    "\n",
    "# Generate a classification report for the utility\n",
    "# report(get_versions_TSSB()[utility][:-4],true_array, pred_array,2)\n",
    "\n",
    "plot_importance_impact('MTF_simple_reverse')\n",
    "df = pd.DataFrame({'Num_of_taken_nodes':np.array(result_array).reshape(81),'No_anomaly': imp_results[0], 'No_recovery':imp_results[1], 'Recovery':imp_results[2], 'Instant':imp_results[3], 'Slow':imp_results[4]})\n",
    "df.to_csv('MTF_simple_reverse.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7139d2-cdd8-42df-86fb-443e11396760",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This for loop runs the main function for graph generation, training and saving the testing results into a csv file.\n",
    "The trained model can be found in DvcLiveLogger/dvclive_run/checkpoints.\n",
    "\"\"\"\n",
    "#Most common Config parameters\n",
    "Config[\"graph\"][\"classif\"] = \"graph\"\n",
    "Config[\"graph\"][\"type\"] = \"MTF\"\n",
    "Config[\"model\"][\"SEED\"] = 280\n",
    "Config[\"importance\"][\"importance\"] = 1\n",
    "Config[\"importance\"][\"ckpt_path\"] = \"Trained_models_Rutgers/MTF_seed=280/version_0.ckpt\" # path to the saved models parameters\n",
    "Config[\"model\"][\"status\"] = \"test_auto\"\n",
    "\n",
    "\n",
    "#For generating, training and testing the graph\n",
    "main()\n",
    "\n",
    "# Generate a classification report for the utility\n",
    "# report(get_versions_TSSB()[utility][:-4],true_array, pred_array,2)\n",
    "\n",
    "plot_importance_impact('MTF_GraphXAI_reverse')\n",
    "df = pd.DataFrame({'Num_of_taken_nodes':np.array(result_array).reshape(81),'No_anomaly': imp_results[0], 'No_recovery':imp_results[1], 'Recovery':imp_results[2], 'Instant':imp_results[3], 'Slow':imp_results[4]})\n",
    "df.to_csv('MTF_GraphXAI_reverse.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
